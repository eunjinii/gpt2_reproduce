{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_window_dilation_pairs(self, sequence_length):\n",
    "    i = 1\n",
    "    pairs = []\n",
    "    while i*4 <= sequence_length:\n",
    "        pairs.append((i*4, i)) # window_size, dilation_rate\n",
    "        i *= self.alpha\n",
    "    return pairs \n",
    "\n",
    "def create_dilated_mask(row_dim, col_dim, dilation_rate, head_index=0, offset=True):\n",
    "    mask = torch.zeros(row_dim, col_dim)\n",
    "    start = (head_index % dilation_rate) if offset else 0\n",
    "    for i in range(start, row_dim, dilation_rate):\n",
    "        for j in range(start, col_dim, dilation_rate):\n",
    "            # if i >= j:\n",
    "            mask[i, j] = 1\n",
    "    return mask\n",
    "\n",
    "def sparseToDense(sparse_tensor, dilation_rate, head_index=0, offset=True):\n",
    "    leading_dims = sparse_tensor.shape[:-2]\n",
    "    s_r, s_c = sparse_tensor.shape[-2], sparse_tensor.shape[-1]\n",
    "    d_r, d_c = s_r // dilation_rate, s_c // dilation_rate\n",
    "    dense_tensor = torch.zeros(*leading_dims, d_r, d_c, device=sparse_tensor.device)\n",
    "    \n",
    "    start = (head_index % dilation_rate) if offset else 0\n",
    "    for i in range(d_r):\n",
    "        for j in range(d_c):\n",
    "            dense_tensor[..., i, j] = sparse_tensor[..., start + i * dilation_rate, start + j * dilation_rate]\n",
    "    return dense_tensor\n",
    "\n",
    "def denseToSparse(dense_tensor, dilation_rate, head_index=0, offset=True):\n",
    "    leading_dims = dense_tensor.shape[:-2]\n",
    "    d_r, d_c = dense_tensor.shape[-2], dense_tensor.shape[-1]\n",
    "    s_r, s_c = d_r * dilation_rate, d_c * dilation_rate\n",
    "    sparse_tensor = torch.zeros(*leading_dims, s_r, s_c, device=dense_tensor.device)\n",
    "    \n",
    "    start = (head_index % dilation_rate) if offset else 0\n",
    "    for i in range(d_r):\n",
    "        for j in range(d_c):\n",
    "            sparse_tensor[..., start + i * dilation_rate, start + j * dilation_rate] = dense_tensor[..., i, j]\n",
    "    return sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.3703,    -inf,    -inf,    -inf],\n",
      "          [ 0.0923,  0.1493,    -inf,    -inf],\n",
      "          [-0.2450,  0.3523,  0.7881,    -inf],\n",
      "          [ 0.4844,  0.4663,  0.5979, -0.2112]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4858, 0.5142, 0.0000, 0.0000],\n",
      "          [0.1777, 0.3230, 0.4993, 0.0000],\n",
      "          [0.2777, 0.2727, 0.3111, 0.1385]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[-0.0072,    -inf,    -inf,    -inf],\n",
      "          [ 0.7520,  0.7695,    -inf,    -inf],\n",
      "          [ 0.3706, -0.5025, -0.0656,    -inf],\n",
      "          [-0.2385, -0.0898,  0.1207,  0.7684]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4956, 0.5044, 0.0000, 0.0000],\n",
      "          [0.4845, 0.2023, 0.3132, 0.0000],\n",
      "          [0.1580, 0.1833, 0.2263, 0.4324]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[-0.0764,    -inf,    -inf,    -inf],\n",
      "          [ 0.1567,  0.4702,    -inf,    -inf],\n",
      "          [-0.0333, -0.4882, -0.1496,    -inf],\n",
      "          [-0.0372,  0.4990,  0.1498,  0.3219]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4223, 0.5777, 0.0000, 0.0000],\n",
      "          [0.3961, 0.2513, 0.3526, 0.0000],\n",
      "          [0.1870, 0.3197, 0.2255, 0.2678]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[-0.1687,    -inf,    -inf,    -inf],\n",
      "          [-0.0452,  0.1558,    -inf,    -inf],\n",
      "          [-0.0256, -0.0670, -0.0149,    -inf],\n",
      "          [ 0.3935,  0.2798,  0.6098,  0.1161]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4499, 0.5501, 0.0000, 0.0000],\n",
      "          [0.3367, 0.3230, 0.3403, 0.0000],\n",
      "          [0.2569, 0.2293, 0.3190, 0.1947]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[ 0.2586,    -inf,    -inf,    -inf],\n",
      "          [-0.2195, -0.5628,    -inf,    -inf],\n",
      "          [-0.0727, -0.0740, -0.1768,    -inf],\n",
      "          [-0.3128, -0.3400, -0.0781, -0.0503]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5850, 0.4150, 0.0000, 0.0000],\n",
      "          [0.3449, 0.3444, 0.3107, 0.0000],\n",
      "          [0.2204, 0.2144, 0.2787, 0.2865]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[-0.0508,    -inf,    -inf,    -inf],\n",
      "          [-0.5429, -1.0911,    -inf,    -inf],\n",
      "          [ 0.0301, -0.0433,  0.0440,    -inf],\n",
      "          [-0.3160,  0.1050,  0.3433,  0.3577]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6337, 0.3663, 0.0000, 0.0000],\n",
      "          [0.3398, 0.3157, 0.3445, 0.0000],\n",
      "          [0.1558, 0.2374, 0.3012, 0.3056]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[-0.5713,    -inf,    -inf,    -inf],\n",
      "          [-0.3618,  0.2387,    -inf,    -inf],\n",
      "          [ 0.8990,  0.6858, -1.1900,    -inf],\n",
      "          [ 0.1560, -0.1276,  0.2181,  0.0704]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3542, 0.6458, 0.0000, 0.0000],\n",
      "          [0.5177, 0.4182, 0.0641, 0.0000],\n",
      "          [0.2677, 0.2016, 0.2849, 0.2458]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[-1.1778,    -inf,    -inf,    -inf],\n",
      "          [-0.1381,  0.0410,    -inf,    -inf],\n",
      "          [-0.2545, -0.0288, -0.2069,    -inf],\n",
      "          [-0.0419, -0.5995,  0.3185,  0.0923]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4554, 0.5446, 0.0000, 0.0000],\n",
      "          [0.3028, 0.3795, 0.3176, 0.0000],\n",
      "          [0.2410, 0.1380, 0.3455, 0.2756]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[ 0.0489,    -inf,    -inf,    -inf],\n",
      "          [ 0.1654, -0.0875,    -inf,    -inf],\n",
      "          [ 0.0527, -0.1168,  0.1354,    -inf],\n",
      "          [-0.4010,  0.2474,  0.1201,  0.1658]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.5629, 0.0000, 0.4371, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.3412, 0.0000, 0.2880, 0.0000, 0.3707, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.1573, 0.0000, 0.3008, 0.0000, 0.2648, 0.0000, 0.2772]]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[[[ 0.0532,    -inf,    -inf,    -inf],\n",
      "          [ 0.0496,  0.0384,    -inf,    -inf],\n",
      "          [ 0.0210,  0.1103, -0.0358,    -inf],\n",
      "          [-0.0100, -0.0920,  0.0336, -0.0192]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.5028, 0.0000, 0.4972, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.3292, 0.0000, 0.3599, 0.0000, 0.3110, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.2527, 0.0000, 0.2328, 0.0000, 0.2640, 0.0000, 0.2504]]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[[[-0.3484,    -inf,    -inf,    -inf],\n",
      "          [-0.0941, -0.0296,    -inf,    -inf],\n",
      "          [-0.6731, -0.5330, -0.5447,    -inf],\n",
      "          [-0.6004, -0.5030, -0.3181,  0.0824]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.4839, 0.0000, 0.5161, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.3042, 0.0000, 0.3499, 0.0000, 0.3459, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.1849, 0.0000, 0.2038, 0.0000, 0.2452, 0.0000, 0.3660]]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[[[ 0.0422,    -inf,    -inf,    -inf],\n",
      "          [-0.0067,  0.0124,    -inf,    -inf],\n",
      "          [-0.0077,  0.0006,  0.0385,    -inf],\n",
      "          [ 0.0294, -0.1118, -0.1938,  0.0694]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.4952, 0.0000, 0.5048, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.3273, 0.0000, 0.3300, 0.0000, 0.3427, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.2696, 0.0000, 0.2341, 0.0000, 0.2157, 0.0000, 0.2806]]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[[[ 0.0170,    -inf,    -inf,    -inf],\n",
      "          [-0.0481,  0.1692,    -inf,    -inf],\n",
      "          [-0.0293,  0.1030,  0.0296,    -inf],\n",
      "          [-0.1149,  0.4040,  0.1162,  0.0046]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.4459, 0.0000, 0.0000, 0.0000, 0.5541, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.3123, 0.0000, 0.0000, 0.0000, 0.3565, 0.0000, 0.0000,\n",
      "           0.0000, 0.3312, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.1974, 0.0000, 0.0000, 0.0000, 0.3316, 0.0000, 0.0000,\n",
      "           0.0000, 0.2487, 0.0000, 0.0000, 0.0000, 0.2224, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[[[-0.0926,    -inf,    -inf,    -inf],\n",
      "          [-0.2275,  0.0811,    -inf,    -inf],\n",
      "          [ 0.3143, -0.1120,  0.0533,    -inf],\n",
      "          [ 0.0272, -0.0097,  0.0046, -0.0305]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.4235, 0.0000, 0.0000, 0.0000, 0.5765, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.4127, 0.0000, 0.0000, 0.0000, 0.2694, 0.0000, 0.0000,\n",
      "           0.0000, 0.3179, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.2574, 0.0000, 0.0000, 0.0000, 0.2480, 0.0000, 0.0000,\n",
      "           0.0000, 0.2516, 0.0000, 0.0000, 0.0000, 0.2430, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[[[ 0.0170,    -inf,    -inf,    -inf],\n",
      "          [-0.0293,  0.0296,    -inf,    -inf],\n",
      "          [-0.0333,  0.0337, -0.0926,    -inf],\n",
      "          [ 0.1130, -0.1143,  0.3143,  0.0533]]]], grad_fn=<CopySlices>)\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<CopySlices>)\n",
      "(tensor([[[ 0.1627, -0.2498, -0.1730,  0.2484],\n",
      "         [ 0.1894,  0.4084, -0.1669, -0.0707],\n",
      "         [ 0.2594,  0.1755,  0.0041, -0.2214],\n",
      "         [ 0.1152, -0.0187, -0.0859, -0.2222],\n",
      "         [ 0.0477, -0.1079,  0.0875, -0.0026],\n",
      "         [-0.2086, -0.0484,  0.0727, -0.2971],\n",
      "         [-0.4360, -0.1531, -0.0844,  0.0239],\n",
      "         [-0.7624, -0.3965,  0.2839,  0.6273],\n",
      "         [-0.6903, -0.5993, -0.4052,  0.1073],\n",
      "         [-0.1651,  0.3255, -0.1694, -0.2956],\n",
      "         [-0.1996, -0.2015, -0.2652, -0.0020],\n",
      "         [-0.1066,  0.0669, -0.1308, -0.2029],\n",
      "         [ 0.1592,  0.3960,  0.0261,  0.0634],\n",
      "         [ 0.1124,  0.0252,  0.1654, -0.0361],\n",
      "         [ 0.1046,  0.0199,  0.0252, -0.0083],\n",
      "         [-0.0393,  0.0611, -0.0505, -0.0543],\n",
      "         [-0.1946, -0.2758,  0.4189,  0.2773],\n",
      "         [-0.0253, -0.3081,  0.3617,  0.6376],\n",
      "         [-0.1546, -0.0925,  0.3403,  0.4292],\n",
      "         [-0.1887, -0.1379,  0.2862,  0.8045],\n",
      "         [-0.1820, -0.2054, -0.0664,  0.2200],\n",
      "         [ 0.0324,  0.0630,  0.2691,  0.7104],\n",
      "         [-0.1087,  0.0339,  0.3778,  0.4452],\n",
      "         [-0.1258, -0.0618,  0.5173,  0.7802],\n",
      "         [ 0.3887, -0.1371,  0.5486,  0.4079],\n",
      "         [-0.3906, -0.0927,  0.3791,  1.2961],\n",
      "         [-0.1531, -0.1656,  0.3581,  0.6069],\n",
      "         [-0.2473, -0.4177,  0.0780,  0.6317],\n",
      "         [ 0.6463,  0.2724,  0.0989,  0.2463],\n",
      "         [-0.1460,  0.0476,  0.3880,  0.9051],\n",
      "         [ 0.0310,  0.0871,  0.1736,  0.2910],\n",
      "         [ 0.2205,  0.2749, -0.0373,  0.3289]]], grad_fn=<DivBackward0>), None, None)\n"
     ]
    }
   ],
   "source": [
    "class MixedDilatedAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.alpha = config.alpha\n",
    "\n",
    "    # attention within a window\n",
    "    def dilated_attention_window(self, partial_q, partial_k, partial_v, window_size, dilation_rate, dropout_p=0.0, is_causal=False):\n",
    "        head_index, window_size, hidden_dim = partial_q.size(-3), partial_q.size(-2), partial_k.size(-1)\n",
    "        scale_factor = 1 / math.sqrt(hidden_dim)\n",
    "        attn_bias = torch.zeros(window_size, window_size, dtype=partial_q.dtype)\n",
    "    \n",
    "        # generate and apply masks to q, k, and v\n",
    "        mask = create_dilated_mask(window_size, hidden_dim, dilation_rate, head_index, offset=True)\n",
    "        masked_q = partial_q * mask\n",
    "        masked_k = partial_k * mask\n",
    "        masked_v = partial_v * mask\n",
    "        \n",
    "        # Apply causal mask if is_causal is True\n",
    "        if is_causal:\n",
    "            causal_mask = torch.tril(torch.ones(window_size, window_size, dtype=torch.bool))\n",
    "            attn_bias.masked_fill_(~causal_mask, float(\"-inf\") )\n",
    "        \n",
    "        attn_weight = torch.matmul(masked_q, masked_k.transpose(-2, -1)) * scale_factor + attn_bias\n",
    "        attn_weight = sparseToDense(attn_weight, dilation_rate, head_index)\n",
    "        \n",
    "        print(attn_weight)\n",
    "        attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "        attn_weight = denseToSparse(attn_weight, dilation_rate, head_index)\n",
    "        print(attn_weight)\n",
    "        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "        \n",
    "        output_hat = attn_weight @ masked_v\n",
    "        output_hat = output_hat * mask # output masking rule\n",
    "        num_row = int(attn_weight.sum(dim=-1).sum().item()) # row that has some values other than zeros\n",
    "        return output_hat, attn_weight, num_row\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch, seq_len, embedding dim (from nanogpt)\n",
    "        head_dim = C // self.n_head\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=-1)\n",
    "\n",
    "        k = k.view(B, T, self.n_head, head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        y = torch.zeros(B, T, C, device=x.device)\n",
    "        denominator = []\n",
    "        wr_pairs = self.make_window_dilation_pairs(sequence_length=T)\n",
    "\n",
    "        for window_size, dilation_rate in wr_pairs: # multiple segment - dilation pairs\n",
    "            partial_denominator = 0\n",
    "            num_windows = T // window_size\n",
    "            # concated_output = torch.zeros(B, self.n_head, T, head_dim, device=x.device)\n",
    "            concated_output = torch.zeros(B, T, C, device=x.device)\n",
    "\n",
    "            for i in range(num_windows): # parallel segment\n",
    "                start = i * window_size\n",
    "                end = start + window_size\n",
    "                \n",
    "                # Slice out the window for q, k, v\n",
    "                partial_q = q[:, :, start:end, :]  # (B, nh, window_size, hs)\n",
    "                partial_k = k[:, :, start:end, :]  # (B, nh, window_size, hs)\n",
    "                partial_v = v[:, :, start:end, :]  # (B, nh, window_size, hs)\n",
    "                \n",
    "                window_output, attn_weight, num_row = self.dilated_attention_window(\n",
    "                    partial_q, partial_k, partial_v, window_size, dilation_rate, is_causal=True\n",
    "                )\n",
    "\n",
    "                # Reshape window_output to (B, window_size, C) for placement in concated_output\n",
    "                window_output = window_output.transpose(1, 2).reshape(B, window_size, C)\n",
    "                concated_output[:, start:end, :] = window_output\n",
    "                partial_denominator += num_row\n",
    "            \n",
    "            denominator.append(partial_denominator)\n",
    "            y += concated_output * partial_denominator\n",
    "  \n",
    "        y /= sum(denominator)\n",
    "        \n",
    "        att_weights, updated_kv_cache = None, None \n",
    "        \n",
    "        return y, att_weights, updated_kv_cache\n",
    "\n",
    "# class Block(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.attn = MixedDilatedAttention(config)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         attn_output, attn_weights, updated_kv_cache = self.attn(x)\n",
    "#         x = x + attn_output\n",
    "#         return x, attn_weights, updated_kv_cache\n",
    "    \n",
    "class Config:\n",
    "    # block_size: int = 16 # max seq_len\n",
    "    n_embd = 4\n",
    "    n_head = 1\n",
    "    alpha = 2\n",
    "\n",
    "config = Config()\n",
    "sequence_length = 32\n",
    "hidden_dim = config.n_embd\n",
    "\n",
    "x = torch.randn(1, sequence_length, hidden_dim)  # Batch size of 1\n",
    "attention_layer = MixedDilatedAttention(config)\n",
    "output = attention_layer(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
