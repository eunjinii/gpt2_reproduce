{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_window_dilation_pairs(alpha, sequence_length):\n",
    "    i = 1\n",
    "    pairs = []\n",
    "    while i*4 <= sequence_length:\n",
    "        pairs.append((i*4, i)) # window_size, dilation_rate\n",
    "        i *= alpha\n",
    "    return pairs \n",
    "\n",
    "def create_dilated_mask(row_dim, col_dim, dilation_rate, head_index=0, offset=True):\n",
    "    mask = torch.zeros(row_dim, col_dim)\n",
    "    start = (head_index % dilation_rate) if offset else 0\n",
    "    for i in range(start, row_dim, dilation_rate):\n",
    "        for j in range(start, col_dim, dilation_rate):\n",
    "            # if i >= j:\n",
    "            mask[i, j] = 1\n",
    "    return mask\n",
    "\n",
    "def sparseToDense(sparse_tensor, dilation_rate, head_index=0, offset=True):\n",
    "    leading_dims = sparse_tensor.shape[:-2]\n",
    "    s_r, s_c = sparse_tensor.shape[-2], sparse_tensor.shape[-1]\n",
    "    d_r, d_c = s_r // dilation_rate, s_c // dilation_rate\n",
    "    dense_tensor = torch.zeros(*leading_dims, d_r, d_c, device=sparse_tensor.device)\n",
    "    \n",
    "    start = (head_index % dilation_rate) if offset else 0\n",
    "    for i in range(d_r):\n",
    "        for j in range(d_c):\n",
    "            dense_tensor[..., i, j] = sparse_tensor[..., start + i * dilation_rate, start + j * dilation_rate]\n",
    "    return dense_tensor\n",
    "\n",
    "def denseToSparse(dense_tensor, dilation_rate, head_index=0, offset=True):\n",
    "    leading_dims = dense_tensor.shape[:-2]\n",
    "    d_r, d_c = dense_tensor.shape[-2], dense_tensor.shape[-1]\n",
    "    s_r, s_c = d_r * dilation_rate, d_c * dilation_rate\n",
    "    sparse_tensor = torch.zeros(*leading_dims, s_r, s_c, device=dense_tensor.device)\n",
    "    \n",
    "    start = (head_index % dilation_rate) if offset else 0\n",
    "    for i in range(d_r):\n",
    "        for j in range(d_c):\n",
    "            sparse_tensor[..., start + i * dilation_rate, start + j * dilation_rate] = dense_tensor[..., i, j]\n",
    "    return sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention time: 1732874600559.0066 ms\n"
     ]
    }
   ],
   "source": [
    "class MixedDilatedAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.alpha = config.alpha\n",
    "        self.wr_pairs = [(4, 1), (8, 2), (16, 4), (32, 8)] # make_window_dilation_pairs(alpha=self.alpha, sequence_length=T)\n",
    "\n",
    "    # attention within a window\n",
    "    def dilated_attention_window(self, partial_q, partial_k, partial_v, window_size, dilation_rate, dropout_p=0.0, is_causal=False):\n",
    "        head_index, window_size, hidden_dim = partial_q.size(-3), partial_q.size(-2), partial_k.size(-1)\n",
    "        scale_factor = 1 / math.sqrt(hidden_dim)\n",
    "        # attn_bias = torch.zeros(window_size, window_size, dtype=partial_q.dtype)\n",
    "    \n",
    "        # generate and apply masks to q, k, and v\n",
    "        mask = create_dilated_mask(window_size, hidden_dim, dilation_rate, head_index, offset=True)\n",
    "        masked_q = partial_q * mask\n",
    "        masked_k = partial_k * mask\n",
    "        masked_v = partial_v * mask\n",
    "        \n",
    "        attn_weight = torch.matmul(masked_q, masked_k.transpose(-2, -1)) * scale_factor\n",
    "        \n",
    "        # Apply causal mask if is_causal is True\n",
    "        if is_causal:\n",
    "            causal_mask = torch.tril(torch.ones(window_size, window_size, dtype=torch.bool))\n",
    "            # attn_bias.masked_fill_(~causal_mask, float(\"-inf\") )\n",
    "            attn_weight = attn_weight.masked_fill_(~causal_mask, float(\"-inf\") )\n",
    "        \n",
    "        \n",
    "        # print(attn_weight)\n",
    "        # attn_weight = sparseToDense(attn_weight, dilation_rate, head_index)\n",
    "        attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "        # attn_weight = denseToSparse(attn_weight, dilation_rate, head_index)\n",
    "        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "        # print(attn_weight[0][0])\n",
    "        \n",
    "        output_hat = attn_weight @ masked_v\n",
    "        output_hat = output_hat * mask # output masking rule\n",
    "        num_row = int(attn_weight.sum(dim=-1).sum().item()) # row that has some values other than zeros\n",
    "        return output_hat, attn_weight, num_row\n",
    "\n",
    "    def forward(self, x):\n",
    "        start = time.time()\n",
    "        B, T, C = x.size() # batch, seq_len, embedding dim (from nanogpt)\n",
    "        head_dim = C // self.n_head\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        q, k, v = [tensor.view(*tensor.shape[:-1], self.n_head, -1).transpose(-3, -2) for tensor in (q, k, v)]\n",
    "        \n",
    "        y = torch.zeros_like(x)\n",
    "        denominator = []\n",
    "\n",
    "        for window_size, dilation_rate in self.wr_pairs: # multiple segment - dilation pairs\n",
    "            partial_denominator = 0\n",
    "            num_windows = T // window_size\n",
    "            concated_output = torch.zeros_like(x)\n",
    "            \n",
    "            # print(num_windows)\n",
    "            for i in range(num_windows): # parallel segment\n",
    "                start = i * window_size\n",
    "                end = start + window_size\n",
    "                \n",
    "                # Slice out the window for q, k, v\n",
    "                partial_q = q[:, :, start:end, :]  # (B, nh, window_size, hs)\n",
    "                partial_k = k[:, :, start:end, :]  # (B, nh, window_size, hs)\n",
    "                partial_v = v[:, :, start:end, :]  # (B, nh, window_size, hs)\n",
    "                window_output, attn_weight, num_row = self.dilated_attention_window(\n",
    "                    partial_q, partial_k, partial_v, window_size, dilation_rate, is_causal=True\n",
    "                )\n",
    "\n",
    "                # Reshape window_output to (B, window_size, C) for placement in concated_output\n",
    "                window_output = window_output.transpose(1, 2).reshape(B, window_size, C)\n",
    "                concated_output[:, start:end, :] = window_output\n",
    "                partial_denominator += num_row\n",
    "            \n",
    "            denominator.append(partial_denominator)\n",
    "            y += concated_output * partial_denominator\n",
    "  \n",
    "        y /= sum(denominator)\n",
    "        \n",
    "        att_weights, updated_kv_cache = None, None \n",
    "        end = time.time()\n",
    "        print(f\"Attention time: {1000*(end - start):.4f} ms\")\n",
    "        \n",
    "        return y, att_weights, updated_kv_cache\n",
    "    \n",
    "class Config:\n",
    "    # block_size: int = 16 # max seq_len\n",
    "    n_embd = 4\n",
    "    n_head = 1\n",
    "    alpha = 2\n",
    "\n",
    "config = Config()\n",
    "sequence_length = 32\n",
    "hidden_dim = config.n_embd\n",
    "\n",
    "x = torch.randn(1, sequence_length, hidden_dim)  # Batch size of 1\n",
    "attention_layer = MixedDilatedAttention(config)\n",
    "output = attention_layer(x)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before split and sparsify torch.Size([1, 32768, 768])\n",
      "After split and sparsify torch.Size([1, 2, 2731, 768])\n",
      "Attention time: 95.6528 ms\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/kyegomez/LongNet/blob/master/long_net/attention.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RelativePositionBias(nn.Module):\n",
    "    def __init__(\n",
    "        self, bidirectional=True, num_buckets=32, max_distance=128, n_heads=12\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_buckets = num_buckets\n",
    "        self.max_distance = max_distance\n",
    "        self.n_heads = n_heads\n",
    "        self.relative_attention_bias = nn.Embedding(\n",
    "            self.num_buckets, self.n_heads\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _relative_position_bucket(\n",
    "        relative_position, bidirectional=True, num_buckets=32, max_distance=128\n",
    "    ):\n",
    "        ret = 0\n",
    "        n = -relative_position\n",
    "        if bidirectional:\n",
    "            num_buckets //= 2\n",
    "            ret += (n < 0).to(torch.long) * num_buckets\n",
    "            n = torch.abs(n)\n",
    "        else:\n",
    "            n = torch.max(n, torch.zeros_like(n))\n",
    "\n",
    "        max_exact = num_buckets // 2\n",
    "        is_small = n < max_exact\n",
    "\n",
    "        val_if_large = max_exact + (\n",
    "            torch.log(n.float() / max_exact)\n",
    "            / math.log(max_distance / max_exact)\n",
    "            * (num_buckets - max_exact)\n",
    "        ).to(torch.long)\n",
    "        val_if_large = torch.min(\n",
    "            val_if_large, torch.full_like(val_if_large, num_buckets - 1)\n",
    "        )\n",
    "\n",
    "        ret += torch.where(is_small, n, val_if_large)\n",
    "        return ret\n",
    "\n",
    "    def compute_bias(self, qlen, klen, step=None):\n",
    "        step = 0 if step is None else step\n",
    "        context_position = torch.arange(\n",
    "            step,\n",
    "            step + qlen,\n",
    "            dtype=torch.long,\n",
    "            device=self.relative_attention_bias.weight.device,\n",
    "        )[:, None]\n",
    "        memory_position = torch.arange(\n",
    "            klen,\n",
    "            dtype=torch.long,\n",
    "            device=self.relative_attention_bias.weight.device,\n",
    "        )[None, :]\n",
    "        relative_position = (\n",
    "            memory_position - context_position\n",
    "        )  # shape (qlen, klen)\n",
    "\n",
    "        rp_bucket = self._relative_position_bucket(\n",
    "            relative_position,  # shape (qlen, klen)\n",
    "            bidirectional=self.bidirectional,\n",
    "            num_buckets=self.num_buckets,\n",
    "            max_distance=self.max_distance,\n",
    "        )\n",
    "        rp_bucket = rp_bucket.to(self.relative_attention_bias.weight.device)\n",
    "        values = self.relative_attention_bias(\n",
    "            rp_bucket\n",
    "        )  # shape (qlen, klen, heads)\n",
    "        values = values.permute([2, 0, 1]).unsqueeze(\n",
    "            0\n",
    "        )  # shape (1, heads, qlen, klen)\n",
    "        return values\n",
    "\n",
    "    def forward(self, batch_size, qlen, klen, step=None):\n",
    "        # shape (batch * heads, qlen, klen)\n",
    "        return (\n",
    "            self.compute_bias(qlen, klen, step)\n",
    "            .repeat(batch_size, 1, 1, 1)\n",
    "            .view(-1, qlen, klen)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def fixed_pos_embedding(x):\n",
    "    seq_len, dim = x.shape\n",
    "    inv_freq = 1.0 / (10000 ** (torch.arange(0, dim) / dim))\n",
    "    sinusoid_inp = torch.einsum(\n",
    "        \"i , j -> i j\", torch.arange(0, seq_len, dtype=torch.float), inv_freq\n",
    "    ).to(x)\n",
    "    return torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)\n",
    "\n",
    "\n",
    "def rotate_every_two(x):\n",
    "    x1 = x[:, :, ::2]\n",
    "    x2 = x[:, :, 1::2]\n",
    "    x = torch.stack((-x2, x1), dim=-1)\n",
    "    return x.flatten(\n",
    "        -2\n",
    "    )  # in einsum notation: rearrange(x, '... d j -> ... (d j)')\\\n",
    "\n",
    "\n",
    "def duplicate_interleave(m):\n",
    "    \"\"\"\n",
    "    A simple version of `torch.repeat_interleave` for duplicating a matrix while interleaving the copy.\n",
    "    \"\"\"\n",
    "    dim0 = m.shape[0]\n",
    "    m = m.view(-1, 1)  # flatten the matrix\n",
    "    m = m.repeat(1, 2)  # repeat all elements into the 2nd dimension\n",
    "    m = m.view(dim0, -1)  # reshape into a matrix, interleaving the copy\n",
    "    return m\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(x, sin, cos, scale=1):\n",
    "    sin, cos = map(lambda t: duplicate_interleave(t * scale), (sin, cos))\n",
    "    # einsum notation for lambda t: repeat(t[offset:x.shape[1]+offset,:], \"n d -> () n () (d j)\", j=2)\n",
    "    return (x * cos) + (rotate_every_two(x) * sin)\n",
    "\n",
    "\n",
    "class XPOS(nn.Module):\n",
    "    def __init__(self, head_dim, scale_base=512):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.scale_base = scale_base\n",
    "        self.register_buffer(\n",
    "            \"scale\",\n",
    "            (torch.arange(0, head_dim, 2) + 0.4 * head_dim) / (1.4 * head_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, offset=0, downscale=False):\n",
    "        length = x.shape[1]\n",
    "        min_pos = -(length + offset) // 2\n",
    "        max_pos = length + offset + min_pos\n",
    "        scale = (\n",
    "            self.scale\n",
    "            ** torch.arange(min_pos, max_pos, 1)\n",
    "            .to(self.scale)\n",
    "            .div(self.scale_base)[:, None]\n",
    "        )\n",
    "        sin, cos = fixed_pos_embedding(scale)\n",
    "\n",
    "        if scale.shape[0] > length:\n",
    "            scale = scale[-length:]\n",
    "            sin = sin[-length:]\n",
    "            cos = cos[-length:]\n",
    "\n",
    "        if downscale:\n",
    "            scale = 1 / scale\n",
    "\n",
    "        x = apply_rotary_pos_emb(x, sin, cos, scale)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# add alibi, qk layer norm, one write head, multihway,\n",
    "class DilatedAttention2(nn.Module):\n",
    "    \"\"\"\n",
    "    Dilated Attention Module.\n",
    "\n",
    "    Arguments:\n",
    "        dim: The dimension of the attention layers.\n",
    "        heads: The number of attention heads.\n",
    "        dilation_rate: The dilation rate for dilated attention.\n",
    "        segment_size: The segment size for dilated attention.\n",
    "        dropout (optional): The dropout probability. Default: 0.0\n",
    "        causal (optional): If set to True, the attention mechanism is causal. Default: False\n",
    "        use_xpos (optional): If set to True, xpos is used for positional encoding. Default: False\n",
    "        use_rel_pos_bias (optional): If set to True, relative position bias is used in the attention mechanism. Default: False\n",
    "\n",
    "    Usage:\n",
    "        The `DilatedAttention` class can be used as a module for neural networks and is especially suited for transformer architectures.\n",
    "\n",
    "        Example:\n",
    "            attention = DilatedAttention(dim=512, heads=8, dilation_rate=2, segment_size=64, use_xpos=True, use_rel_pos_bias=True)\n",
    "            output = attention(input_tensor)\n",
    "\n",
    "        This will return the output tensor after applying dilated attention. The `use_xpos` and `use_rel_pos_bias` parameters allow for switching on positional encoding and relative positional bias respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        heads: int,\n",
    "        dilation_rate: int,\n",
    "        segment_size: int,\n",
    "        # dropout: float = 0.0,\n",
    "        causal: bool = False,\n",
    "        use_xpos: bool = False,\n",
    "        use_rel_pos_bias: bool = False,\n",
    "        qk_norm: bool = False,\n",
    "        dtype: torch.dtype = torch.float16,\n",
    "        device: str = \"cuda:0\",\n",
    "    ) -> None:\n",
    "        super(DilatedAttention2, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.segment_size = segment_size\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        self.causal = causal\n",
    "        self.use_xpos = use_xpos\n",
    "        self.use_rel_pos_bias = use_rel_pos_bias\n",
    "        self.qk_norm = qk_norm\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "\n",
    "        # self.attention = FlashAttention(causal=self.causal, dropout=dropout).to(device)\n",
    "\n",
    "        # if use_xpos:\n",
    "        #     self.xpos = XPOS(head_dim=dim // heads)\n",
    "        # if use_rel_pos_bias:\n",
    "        #     self.relative_bias = RelativePositionBias(\n",
    "        #         num_buckets=32, max_distance=128, n_heads=heads\n",
    "        #     )\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        # head offsets\n",
    "        self.head_offsets = nn.Parameter(torch.randn(heads, dim))\n",
    "\n",
    "        # Linear Projections\n",
    "        self.proj_q = nn.Linear(dim, dim)\n",
    "        self.proj_k = nn.Linear(dim, dim)\n",
    "        self.proj_v = nn.Linear(dim, dim)\n",
    "\n",
    "    def get_mask(self, i, j):\n",
    "        \"\"\"i = row, j=column\"\"\"\n",
    "        return torch.ones((i, j), device=self.device, dtype=torch.bool).triu(\n",
    "            j - i + 2\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the DilatedAttention module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        padding_len = -seq_len % self.segment_size\n",
    "        x = F.pad(x, (0, 0, 0, padding_len))\n",
    "        seq_len = seq_len + padding_len\n",
    "\n",
    "        # if self.use_xpos:\n",
    "        #     x = self.xpos(x)\n",
    "        print('Before split and sparsify', x.shape)\n",
    "\n",
    "        # Split and sparsify\n",
    "        x = x.view(batch_size, -1, self.segment_size, self.dim)\n",
    "        x = x[:, :, :: self.dilation_rate, :]\n",
    "        \n",
    "        print('After split and sparsify', x.shape)\n",
    "\n",
    "        # qk_norm\n",
    "        if self.qk_norm:\n",
    "            q, k, v = map(\n",
    "                self.norm, (self.proj_q(x), self.proj_k(x), self.proj_v(x))\n",
    "            )\n",
    "        else:\n",
    "            q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n",
    "\n",
    "        # Perform attention\n",
    "        # attn_output = self.attention(q, k, v)\n",
    "        attn_output = F.scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "        # if use rel pos => apply relative positioning bias\n",
    "        # if self.use_rel_pos_bias:\n",
    "        #     attn_output += self.relative_bias(\n",
    "        #         batch_size, attn_output.size(1), attn_output.size(1)\n",
    "        #     )\n",
    "\n",
    "        # if causal create a mask and apply to the output\n",
    "        if self.causal:\n",
    "            mask = self.get_mask(attn_output.size(1), attn_output.size(1))\n",
    "\n",
    "            attn_output = attn_output.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "        # apply dropout\n",
    "        # attn_output = self.dropout(attn_output)\n",
    "        # Scatter and concatenate\n",
    "        attn_output = attn_output.reshape(batch_size, -1, self.dim)\n",
    "\n",
    "        end = time.time()\n",
    "        print(f\"Attention time: {1000*(end - start):.4f} ms\")\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "sequence_length = 32768\n",
    "hidden_dim = 768\n",
    "x = torch.randn(1, sequence_length, hidden_dim)  # Batch size of 1\n",
    "attention_layer = DilatedAttention2(\n",
    "\tdim=768, heads=12, dilation_rate=6, segment_size=16384, use_xpos=False, use_rel_pos_bias=False, qk_norm=False\n",
    ")\n",
    "output = attention_layer(x)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
