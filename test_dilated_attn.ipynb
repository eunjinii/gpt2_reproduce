{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dilated_mask(row_dim, col_dim, dilation_rate, head_index=0, offset=True):\n",
    "    mask = torch.zeros(row_dim, col_dim)\n",
    "    start = (head_index % dilation_rate) if offset else 0\n",
    "    for i in range(start, row_dim, dilation_rate):\n",
    "        for j in range(start, col_dim, dilation_rate):\n",
    "            # if i >= j:\n",
    "            mask[i, j] = 1\n",
    "    return mask\n",
    "\n",
    "def sparseToDense(sparse_tensor, dilation_rate, head_index=0, offset=True):\n",
    "    s_r, s_c = sparse_tensor.size()\n",
    "    d_r, d_c = s_r // dilation_rate, s_c // dilation_rate \n",
    "    dense_tensor = torch.zeros(d_r, d_c)\n",
    "    start = (head_index % dilation_rate) if offset else 0\n",
    "    for i in range(d_r):\n",
    "        for j in range(d_c):\n",
    "            dense_tensor[i, j] = sparse_tensor[start+i*dilation_rate][start+j*dilation_rate]\n",
    "    return dense_tensor\n",
    "\n",
    "def denseToSparse(dense_tensor, dilation_rate, head_index=0, offset=True):\n",
    "    d_r, d_c = dense_tensor.size()\n",
    "    s_r, s_c = d_r * dilation_rate, d_c * dilation_rate\n",
    "    sparse_tensor = torch.zeros(s_r, s_c)\n",
    "    start = (head_index % dilation_rate) if offset else 0\n",
    "    for i in range(d_r):\n",
    "        for j in range(d_c):\n",
    "            sparse_tensor[start + i * dilation_rate, start + j * dilation_rate] = dense_tensor[i, j]\n",
    "    return sparse_tensor\n",
    "\n",
    "# def create_dilated_mask(row_dim, col_dim, dilation_rate, head_index=0, offset=True): # paper-based\n",
    "#     mask = torch.zeros(row_dim, col_dim)\n",
    "#     start = (head_index % dilation_rate) if offset else 0\n",
    "#     for i in range(start, row_dim, dilation_rate):\n",
    "#         mask[i, :] = 1  # Select every `dilation_rate`-th row\n",
    "#     return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5161, 0.0000, 0.4839, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2660, 0.0000, 0.4631, 0.0000, 0.2709, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4051, 0.0000, 0.0932, 0.0000, 0.3941, 0.0000, 0.1076, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0317,  0.0000, -0.0871,  0.0000, -1.8382,  0.0000, -1.4081,  0.0000,\n",
       "         -2.5893,  0.0000, -2.0640,  0.0000, -0.9550,  0.0000, -0.1931,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0381,  0.0000,  0.6472,  0.0000,  0.0804,  0.0000, -0.5493,  0.0000,\n",
       "         -0.8101,  0.0000, -1.9289,  0.0000, -1.4728,  0.0000, -0.7911,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.1882,  0.0000,  0.1388,  0.0000,  0.4842,  0.0000, -0.1644,  0.0000,\n",
       "         -0.1446,  0.0000, -1.4414,  0.0000, -1.5465,  0.0000, -1.1938,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.1758,  0.0000, -0.6870,  0.0000, -0.4995,  0.0000, -0.5585,  0.0000,\n",
       "         -0.7237,  0.0000, -1.0515,  0.0000, -1.2002,  0.0000, -0.9398,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from torch.nn import functional as F\n",
    "\n",
    "dilation_rate = 2\n",
    "window_size = 8\n",
    "hidden_dim = 16\n",
    "offset = True\n",
    "head_index = 0\n",
    "\n",
    "partial_q = torch.randn(window_size, hidden_dim)\n",
    "partial_k = torch.randn(window_size, hidden_dim)\n",
    "partial_v = torch.randn(window_size, hidden_dim)\n",
    "\n",
    "# attention within a window\n",
    "def dilated_attention_window(partial_q, partial_k, partial_v, dilation_rate, head_index=0, dropout_p=0.0, is_causal=False):\n",
    "    w, d = partial_q.size(-2), partial_k.size(-1)\n",
    "    scale_factor = 1 / math.sqrt(d)\n",
    "    attn_bias = torch.zeros(w, w, dtype=partial_q.dtype)\n",
    " \n",
    "    # generate and apply masks to q, k, and v\n",
    "    mask = create_dilated_mask(w, d, dilation_rate, head_index, offset=True)\n",
    "    masked_q = partial_q * mask\n",
    "    masked_k = partial_k * mask\n",
    "    masked_v = partial_v * mask\n",
    "    \n",
    "    # Apply causal mask if is_causal is True\n",
    "    if is_causal:\n",
    "        causal_mask = torch.tril(torch.ones(w, w, dtype=torch.bool))\n",
    "        attn_bias.masked_fill_(~causal_mask, float(\"-inf\") )\n",
    "    \n",
    "    attn_weight = torch.matmul(masked_q, masked_k.transpose(-2, -1)) * scale_factor + attn_bias\n",
    "    # print(attn_weight)\n",
    "    attn_weight = sparseToDense(attn_weight, dilation_rate, head_index)\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    attn_weight = denseToSparse(attn_weight, dilation_rate, head_index)\n",
    "    print(attn_weight)    \n",
    "    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    \n",
    "    output_hat = attn_weight @ masked_v\n",
    "    output_hat = output_hat * mask # output masking rule\n",
    "    return output_hat\n",
    "\n",
    "dilated_attention_window(partial_q, partial_k, partial_v, dilation_rate, head_index, is_causal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
