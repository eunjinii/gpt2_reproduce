{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dilated_mask(row_dim, col_dim, dilation_rate, head_index=0, offset=True):\n",
    "    mask = torch.zeros(row_dim, col_dim)\n",
    "    start = (head_index % dilation_rate) if offset else 0\n",
    "    for i in range(start, row_dim, dilation_rate):\n",
    "        for j in range(start, col_dim, dilation_rate):\n",
    "            # if i >= j:\n",
    "            mask[i, j] = 1\n",
    "    return mask\n",
    "\n",
    "def sparseToDense(sparse_tensor, dilation_rate, head_index=0, offset=True):\n",
    "    s_r, s_c = sparse_tensor.size()\n",
    "    d_r, d_c = s_r // dilation_rate, s_c // dilation_rate \n",
    "    dense_tensor = torch.zeros(d_r, d_c)\n",
    "    start = (head_index % dilation_rate) if offset else 0\n",
    "    for i in range(d_r):\n",
    "        for j in range(d_c):\n",
    "            dense_tensor[i, j] = sparse_tensor[start+i*dilation_rate][start+j*dilation_rate]\n",
    "    return dense_tensor\n",
    "\n",
    "def denseToSparse(dense_tensor, dilation_rate, head_index=0, offset=True):\n",
    "    d_r, d_c = dense_tensor.size()\n",
    "    s_r, s_c = d_r * dilation_rate, d_c * dilation_rate\n",
    "    sparse_tensor = torch.zeros(s_r, s_c)\n",
    "    start = (head_index % dilation_rate) if offset else 0\n",
    "    for i in range(d_r):\n",
    "        for j in range(d_c):\n",
    "            sparse_tensor[start + i * dilation_rate, start + j * dilation_rate] = dense_tensor[i, j]\n",
    "    return sparse_tensor\n",
    "\n",
    "# def create_dilated_mask(row_dim, col_dim, dilation_rate, head_index=0, offset=True): # paper-based\n",
    "#     mask = torch.zeros(row_dim, col_dim)\n",
    "#     start = (head_index % dilation_rate) if offset else 0\n",
    "#     for i in range(start, row_dim, dilation_rate):\n",
    "#         mask[i, :] = 1  # Select every `dilation_rate`-th row\n",
    "#     return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.nn import functional as F\n",
    "\n",
    "dilation_rate = 2\n",
    "window_size = 8\n",
    "hidden_dim = 16\n",
    "offset = True\n",
    "head_index = 0\n",
    "\n",
    "# attention within a window\n",
    "def dilated_attention_window(partial_q, partial_k, partial_v, window_size, dilation_rate, head_index=0, dropout_p=0.0, is_causal=False):\n",
    "    window_size, hidden_dim = partial_q.size(-2), partial_k.size(-1)\n",
    "    scale_factor = 1 / math.sqrt(d)\n",
    "    attn_bias = torch.zeros(window_size, window_size, dtype=partial_q.dtype)\n",
    " \n",
    "    # generate and apply masks to q, k, and v\n",
    "    mask = create_dilated_mask(window_size, hidden_dim, dilation_rate, head_index, offset=True)\n",
    "    masked_q = partial_q * mask\n",
    "    masked_k = partial_k * mask\n",
    "    masked_v = partial_v * mask\n",
    "    \n",
    "    # Apply causal mask if is_causal is True\n",
    "    if is_causal:\n",
    "        causal_mask = torch.tril(torch.ones(window_size, window_size, dtype=torch.bool))\n",
    "        attn_bias.masked_fill_(~causal_mask, float(\"-inf\") )\n",
    "    \n",
    "    attn_weight = torch.matmul(masked_q, masked_k.transpose(-2, -1)) * scale_factor + attn_bias\n",
    "    # print(attn_weight)\n",
    "    attn_weight = sparseToDense(attn_weight, dilation_rate, head_index)\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    attn_weight = denseToSparse(attn_weight, dilation_rate, head_index)\n",
    "    # print(attn_weight)    \n",
    "    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    \n",
    "    output_hat = attn_weight @ masked_v\n",
    "    output_hat = output_hat * mask # output masking rule\n",
    "    num_row = int(attn_weight.sum(dim=-1).sum().item()) # row that has some values other than zeros\n",
    "    return output_hat, num_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5223e-01, -6.3395e-01, -6.7170e-01,  8.3738e-01,  3.9974e-01,\n",
      "          2.1473e-01,  9.1315e-01,  2.0024e-02,  1.1435e+00, -6.5006e-02,\n",
      "          2.2309e-01,  1.8378e-01,  3.5100e-01,  1.2653e-01, -1.6455e-01,\n",
      "         -3.6550e-01],\n",
      "        [-2.6500e-01, -2.4508e-01, -3.3913e-01,  5.7264e-01, -1.2404e-01,\n",
      "          1.8270e-01,  4.1402e-01, -3.1299e-02,  6.5652e-01, -5.9528e-01,\n",
      "          2.7515e-01, -2.8199e-01,  1.1703e-01,  1.3926e-01, -2.6042e-01,\n",
      "         -4.4480e-02],\n",
      "        [-9.3077e-02,  3.4226e-01,  2.7646e-01,  1.7710e-01, -8.6060e-02,\n",
      "          8.2148e-02, -3.7113e-01, -1.8295e-01,  9.1809e-01, -9.4049e-01,\n",
      "          1.1124e-03, -3.8898e-01, -9.6351e-02, -2.7345e-02, -5.5050e-01,\n",
      "          5.4099e-01],\n",
      "        [-2.3615e-01, -2.0461e-01, -2.8226e-01,  5.7399e-01, -8.3917e-02,\n",
      "          1.9291e-01,  3.6596e-01, -1.0659e-02,  5.4944e-01, -5.6472e-01,\n",
      "          2.3974e-01, -2.5444e-01,  7.1538e-02,  1.1438e-01, -1.9084e-01,\n",
      "         -4.7025e-02],\n",
      "        [-4.6351e-01,  6.3483e-01, -1.1152e-01, -8.8137e-02,  3.7480e-01,\n",
      "          7.0729e-01,  3.8720e-01, -2.2272e-01,  1.1754e-01,  2.0441e-01,\n",
      "         -9.6440e-02, -1.6541e+00,  3.0101e-02, -2.6312e-01, -1.2879e-01,\n",
      "          1.2190e-01],\n",
      "        [-2.0523e-01,  5.6506e-01,  8.4578e-03, -2.1666e-02,  1.6254e-01,\n",
      "          6.0480e-01,  2.9511e-01, -3.0096e-01, -3.7123e-01,  1.0165e-01,\n",
      "         -9.0104e-02, -1.2306e+00, -4.9530e-02, -1.9717e-01, -1.0694e-01,\n",
      "          1.0141e-01],\n",
      "        [-3.7134e-01, -2.4670e-01,  8.7518e-01,  5.7635e-02, -5.5050e-01,\n",
      "          5.3789e-02, -4.5181e-02, -2.0498e-01, -2.3775e-02, -2.0967e-01,\n",
      "         -5.0012e-01,  4.2927e-01,  2.5833e-01, -2.3503e-01, -5.2692e-01,\n",
      "         -2.7337e-01],\n",
      "        [-1.7298e-01,  9.2474e-02,  5.2570e-01,  1.6893e-01, -5.3694e-01,\n",
      "          1.4506e-01,  3.7257e-02, -4.9397e-01, -6.5499e-01, -2.9833e-01,\n",
      "         -3.3538e-01,  4.1665e-01,  1.2644e-01, -8.0972e-02, -2.4864e-01,\n",
      "         -1.5365e-01],\n",
      "        [-7.1260e-01,  7.0956e-02, -1.7207e-01,  5.4324e-01, -1.0160e-01,\n",
      "         -5.0595e-01,  9.9903e-03, -2.1709e-01,  1.7085e+00, -4.7804e-01,\n",
      "         -2.4919e-02,  2.1475e-02, -1.1990e+00, -4.1059e-02,  9.5249e-01,\n",
      "          1.1226e-01],\n",
      "        [-6.0128e-01,  6.1823e-02, -4.1422e-02, -2.5701e-02, -4.4839e-01,\n",
      "         -1.4560e-01, -5.6249e-01,  2.5069e-01, -4.2319e-01,  4.5204e-01,\n",
      "          3.3431e-02, -5.8356e-02,  2.3645e-01,  2.6969e-01, -1.7821e-01,\n",
      "          7.1725e-01],\n",
      "        [-1.0476e+00,  5.1430e-01,  1.2127e-01, -7.6939e-02,  1.9681e-01,\n",
      "         -4.8967e-01, -9.3894e-01, -2.5750e-01,  7.1508e-01, -2.0210e-01,\n",
      "         -7.9603e-02,  1.4328e-01,  3.1738e-02,  2.2982e-01,  2.3419e-01,\n",
      "          5.8834e-01],\n",
      "        [-1.9853e-01,  3.2016e-01, -5.4114e-01,  6.1058e-02,  1.3117e-01,\n",
      "          3.1959e-02,  6.8421e-01,  2.9151e-01,  3.1519e-01,  1.4858e-01,\n",
      "         -1.1989e-01,  4.0647e-01, -2.2427e-01,  4.1354e-01, -2.0573e-01,\n",
      "         -4.3748e-01],\n",
      "        [-1.0420e+00, -1.3516e-01,  4.0119e-01,  1.0878e-01,  4.2569e-01,\n",
      "         -6.7343e-01,  4.0178e-01, -1.6386e-01, -2.3512e-01, -4.6375e-01,\n",
      "          1.0950e-01,  4.9282e-01,  1.9131e-01,  6.8027e-01, -5.0169e-02,\n",
      "         -6.7425e-01],\n",
      "        [-5.3507e-01,  4.6919e-01,  1.5679e-01,  6.5863e-01,  2.6500e-02,\n",
      "         -7.3223e-02,  4.6430e-01, -3.5341e-01,  4.2911e-01, -4.4862e-01,\n",
      "          1.3728e-02,  3.0878e-01, -4.1072e-03, -2.3002e-02, -4.2057e-02,\n",
      "         -2.7426e-01],\n",
      "        [-3.0560e-01, -1.8471e-01, -1.2132e-01, -1.0896e-01,  2.3414e-01,\n",
      "         -3.3629e-01,  9.6808e-02, -3.9399e-01,  3.2667e-01,  6.7968e-02,\n",
      "         -1.7524e-02,  1.4781e-01, -1.5076e-01,  3.1163e-01, -4.9823e-01,\n",
      "          4.7925e-01],\n",
      "        [-3.0146e-02, -9.9678e-02,  6.4409e-02, -1.8611e-01,  7.9737e-02,\n",
      "         -2.0448e-01,  2.3886e-01, -3.9950e-01, -1.4864e-01,  1.2039e-01,\n",
      "         -2.1608e-01, -5.3740e-02, -1.6178e-01,  3.0971e-01, -4.8180e-01,\n",
      "          6.3619e-01],\n",
      "        [ 8.5659e-01,  7.4423e-01,  6.5592e-01,  1.3936e+00,  1.1828e-01,\n",
      "         -1.0134e+00,  5.0529e-01, -2.2013e-01,  3.3236e-01,  1.2063e+00,\n",
      "          2.4542e-02, -7.4084e-01, -3.4692e-01, -2.4013e-01,  5.8353e-02,\n",
      "          3.9605e-01],\n",
      "        [ 5.5231e-01,  4.5278e-01,  4.4080e-01,  9.0923e-01,  3.8636e-01,\n",
      "         -6.1958e-01,  1.3802e-01, -1.0506e-01,  4.4957e-02,  1.0328e+00,\n",
      "          3.5267e-01, -5.0928e-01, -2.1183e-01, -2.4171e-01, -1.6824e-01,\n",
      "          3.4091e-01],\n",
      "        [ 4.4850e-01,  6.1940e-01,  5.6717e-01,  1.0199e+00,  2.4519e-01,\n",
      "         -5.6684e-01,  4.5229e-01, -2.4781e-01,  3.2996e-01,  6.8200e-01,\n",
      "          3.0116e-01, -4.6420e-01, -2.6264e-01, -9.1595e-02, -3.4574e-01,\n",
      "          2.4883e-01],\n",
      "        [-3.4246e-01,  1.9711e-01,  4.3009e-02,  1.6006e-01, -3.1321e-01,\n",
      "          3.8548e-01, -5.2585e-02, -2.9246e-01,  2.9952e-01, -5.8950e-01,\n",
      "         -2.8698e-01,  2.0539e-01,  5.6924e-02,  2.4324e-01,  1.0590e-01,\n",
      "         -1.6920e-01],\n",
      "        [ 3.6867e-01,  4.4886e-01, -3.9806e-01, -2.0769e-01,  4.2801e-01,\n",
      "         -5.7334e-01, -8.6622e-02,  1.0358e-01, -1.7693e-01,  9.5188e-01,\n",
      "          1.0208e+00,  5.7881e-01, -7.7443e-01, -8.0743e-01,  1.2631e+00,\n",
      "         -6.1360e-01],\n",
      "        [ 3.5585e-01,  3.7182e-01, -6.5064e-01,  1.7928e-01,  3.8071e-01,\n",
      "         -3.5063e-01, -9.9251e-02,  9.7052e-02, -2.4674e-01,  6.6189e-01,\n",
      "          8.2079e-01,  5.4818e-01, -5.5164e-01, -7.2811e-01,  9.7063e-01,\n",
      "         -3.7750e-01],\n",
      "        [ 7.2959e-01,  1.8544e-01,  7.0479e-02,  5.4140e-01,  1.0763e-01,\n",
      "          3.3958e-01, -3.5401e-01, -4.6216e-01, -5.0122e-02, -6.0022e-01,\n",
      "          6.8776e-01,  2.7558e-01, -1.7707e-01, -2.2736e-01,  8.1334e-01,\n",
      "         -3.5167e-01],\n",
      "        [ 2.9739e-01,  1.1339e-01, -5.7636e-01,  2.8464e-01,  1.9084e-01,\n",
      "         -8.4056e-02, -2.1423e-01, -1.4244e-01, -1.0757e-01,  2.1640e-01,\n",
      "          6.3355e-01,  3.8157e-01, -3.4089e-01, -6.3939e-01,  6.8452e-01,\n",
      "         -2.5362e-01],\n",
      "        [-7.4677e-02, -1.2034e-01,  1.1259e+00, -1.2041e-01,  1.6961e-01,\n",
      "          4.5750e-02, -1.4187e+00, -8.9149e-02, -6.1503e-01,  1.1406e-01,\n",
      "         -4.2766e-01,  3.0217e-01, -7.6982e-01, -4.8782e-01, -8.7854e-02,\n",
      "          9.6154e-02],\n",
      "        [ 3.1544e-01, -1.1978e-01,  4.4515e-01, -1.9536e-01,  2.3121e-01,\n",
      "         -2.4947e-01, -3.2523e-02, -3.4188e-01, -2.8786e-01,  1.2059e-01,\n",
      "          2.0683e-02, -2.5688e-01,  4.9754e-02, -6.3270e-01, -5.2280e-02,\n",
      "          7.7046e-02],\n",
      "        [ 4.0568e-02, -2.0898e-02, -3.0131e-01,  1.0486e-01, -9.3285e-01,\n",
      "         -2.1670e-01,  1.4900e-02, -2.6937e-01, -2.1224e-01, -2.3359e-01,\n",
      "         -3.8442e-01, -8.6084e-01, -8.1256e-02, -9.6267e-01, -4.5540e-01,\n",
      "          8.1614e-02],\n",
      "        [ 1.8479e-01, -7.6734e-02,  4.8401e-01, -8.9341e-02,  3.6178e-01,\n",
      "          6.4332e-02, -8.5243e-01, -1.2224e-01, -2.0239e-01,  2.9997e-02,\n",
      "         -2.5872e-01,  1.0914e-01, -1.5878e-01, -6.1620e-01,  5.6819e-02,\n",
      "          6.7564e-02],\n",
      "        [ 8.9860e-01, -7.8107e-01,  5.4826e-03,  7.9427e-01, -4.8077e-01,\n",
      "         -6.6597e-01, -5.7056e-01,  4.9878e-01,  1.3324e-01,  1.1615e-01,\n",
      "         -4.9008e-01,  2.6759e-01, -6.0070e-01,  4.2068e-01,  8.1391e-01,\n",
      "         -1.8106e-01],\n",
      "        [-4.4867e-01, -9.9110e-01, -1.0242e-01,  2.5584e-01, -8.5113e-01,\n",
      "          2.2327e-01,  4.7535e-01,  1.1397e-01,  2.3863e-01, -2.0701e-01,\n",
      "         -2.7189e-01,  3.8929e-01, -3.6016e-01, -9.5286e-01, -5.2228e-01,\n",
      "         -2.7902e-02],\n",
      "        [ 4.5192e-01, -5.3210e-02,  4.8300e-01,  2.2490e-01, -4.1030e-01,\n",
      "         -5.4947e-01,  4.0885e-01,  1.8972e-01,  6.7725e-01,  1.4847e-01,\n",
      "          3.7769e-01, -6.7660e-02, -2.4813e-01, -1.2744e-01,  7.4456e-01,\n",
      "         -3.1966e-01],\n",
      "        [ 2.6094e-01,  1.6474e-01,  2.3627e-01,  6.6802e-02, -2.7491e-01,\n",
      "         -5.7524e-01,  4.5643e-01,  1.3162e-01,  7.0419e-01,  1.7053e-01,\n",
      "          7.9904e-01, -1.6499e-01, -8.4047e-02, -1.7869e-01,  8.9844e-01,\n",
      "         -3.5388e-01]])\n"
     ]
    }
   ],
   "source": [
    "def make_window_dilation_pairs(sequence_length=32, alpha=2):\n",
    "    i = 1\n",
    "    pairs = []\n",
    "    while i*4 <= sequence_length:\n",
    "        pairs.append((i*4, i)) # window_size, dilation_rate\n",
    "        i *= alpha\n",
    "    return pairs \n",
    "\n",
    "class MixedDilatedAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, sequence_length, hidden_dim = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        output = torch.zeros(sequence_length, hidden_dim)\n",
    "        denominator = []\n",
    "        wr_pairs = make_window_dilation_pairs()\n",
    "\n",
    "        for window_size, dilation_rate in wr_pairs: # multiple segment - dilation pairs\n",
    "            partial_denominator = 0\n",
    "            num_windows = sequence_length // window_size\n",
    "            concated_output = torch.zeros(sequence_length, hidden_dim)\n",
    "\n",
    "            for i in range(num_windows): # parallel segment\n",
    "                start = i * window_size\n",
    "                end = start + window_size\n",
    "                \n",
    "                window_output, num_row = dilated_attention_window(partial_q, partial_k, partial_v, window_size, dilation_rate, head_index, is_causal=True)\n",
    "                concated_output[start:end] = window_output\n",
    "                partial_denominator += num_row\n",
    "\n",
    "            denominator.append(partial_denominator)\n",
    "            output += concated_output * partial_denominator\n",
    "\n",
    "        output /= sum(denominator)\n",
    "        return output\n",
    "\n",
    "x = torch.randn(sequence_length, hidden_dim)\n",
    "output = dilated_attention(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1927698/3000626921.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mwr_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_window_dilation_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwr_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "output = torch.zeros(sequence_length, hidden_dim)\n",
    "denominator = 0\n",
    "for w, r in wr_pairs:\n",
    "    concated_output, partial_denominator = construct_full_attention_matrix(sequence_length, window_size=w, dilation_rate=r)\n",
    "    output += concated_output * denominator\n",
    "    denominator += partial_denominator\n",
    "output /= denominator\n",
    "\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
